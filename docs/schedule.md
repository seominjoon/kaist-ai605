| # | Date      | Topics                                  | Assignments | Reading List |
|----------------|-----------|-------------------------------------------------|------------------|-------------------------|
|             01 |  3/3 | Intro to NLP, Deep Learning Basics [[slides][s01]]                     |                ||
|             02 |  3/8 | Deep Learning Basics (2), Tokenization, Word Embedding, Text Classification [[slides][s02]]                      | | [MNIST][mnist], [Mikolov et al. (2014)][word2vec]                       |
|             03 | 3/10 | Text Classification, Tokenization, Word Embedding, Recurrent Neural Networks [[slides][s03]]                      |                |                         |
|             04 | 3/15 | Training, LSTM [[slides][s04]] |                                    | |
|             05 | 3/17 | Jupyter Notebook, Token Classification, NER, MRC [[slides][s05]] |  [Assignment 1][a1] is up                                    |[Vanishing Gradients and Fancy RNNs][cs224n-07]|
|             06 | 3/22 | Text Generation (Machine Translation, Summarization), Encoder-Decoder, Decoder Attention [[slides][s06]] |                                     | [Cho et al. (2014)][seq2seq], [Bahdanau et al. (2015)][att]                         |
|             07 | 3/24 | Encoder Attention, Transformer |                     |                                         |
|             08 | 3/29 | Transformer (2) | |                             |
|             09 | 3/31 | Teacher Forcing, Beam Search, Byte Pair Encoding (BPE) |  Assignment 1 is due, Assignment 2 is up               |                         |
|             10 |  4/5 | NLP Paper analysis |                                     |                         |
|             11 |  4/7 | NLP Paper discussions |                                     |                         |
|             12 | 4/12 | Language Model |                                     |                         |
|             13 | 4/14 | Neural Language Model | Assignment 2 is due, Assignment 3 is up                            |                         |
|             14 | 4/19 | Masked Language Model |                                     |                         |
|             15 | 4/21 | Pretrained Language Model (1) |                                     |                         |
|             16 | 4/26 | Pretrained Language Model (2) |                                     |                         |
|             17 | 4/28 | NLP Tools | Assignment 3 is due, Assignment 4 is up |                            |
|             18 |  5/3 | Assignment 3 presentation |                     |                                         |
|                |  5/5 | No lecture (어린이날)                           |                                     |                         |
|             19 | 5/10 | Intro to the Final Project (Open-domain QA) |                     |                                        |
|             20 | 5/12 | Intro to the Final Project (Open-domain QA) (2) |                                     |                         |
|             21 | 5/17 | (spare) | Assignment 4 is due |                |                      
|                | 5/19 | No lecture (석가탄신일)                         |                                   |                         |
|             22 | 5/24 | Large Language Model (1) |                     |                                       |
|             23 | 5/26 | Large Language Model (2) |                     |                                        |
|             24 | 5/31 | Generalization and In-context Learning     |                                   |                         |
|             25 |  6/2 | Recent trend in NLP                                                                   |                |                         |
|             26 |  6/7 | Final project presentation                      |                                    |                         |
|             27 |  6/9 | FInal project presentation                      |                                   |                         |

[s01]: https://drive.google.com/file/d/1x5E7gCnYaIkHWsy9rzENnTiXnW0pbNfB/view?usp=sharing
[s02]: https://drive.google.com/file/d/1Z2jxgwZFLJzehFCGuIvkBMUIQkvgJGvV/view?usp=sharing
[s03]: https://drive.google.com/file/d/1eKMxk6hv7HSzlMOWSp_lUa3Qsy8pMcZ8/view?usp=sharing
[s04]: https://drive.google.com/file/d/1KaCsDCNnrN9z8CxQlQ_XuKyDI-i9g2Rg/view?usp=sharing
[s05]: https://drive.google.com/file/d/1rANZenSNZSgBs0-9mTNRv1ASlnM4ltHv/view?usp=sharing
[s06]: https://drive.google.com/file/d/1XOv_rHZsxGbCdo-gRummQhOJm0nfITl1/view?usp=sharing
[a1]: https://colab.research.google.com/drive/1SrYqfgY7mFqolA6_fpH6nkCzUOTanmsA?usp=sharing
[word2vec]: https://arxiv.org/abs/1301.3781
[mnist]: http://yann.lecun.com/exdb/mnist/
[cs224n-07]: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture07-fancy-rnn.pdf
[seq2seq]: https://arxiv.org/abs/1406.1078
[att]: https://arxiv.org/abs/1409.0473
